{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Validation\n",
    "\n",
    "It is important to know whether the model that we produce is doing well, do we must evaluate and vlidate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Regression and  Classification Recap\n",
    "\n",
    "This lesson will use the concepts of Regression and Classification frequenly, so to recap:\n",
    "- Regression predicts a *Value* (eg. 4)\n",
    "- Classification predicts a *State* (eg. Dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basic Overfitting\n",
    "\n",
    "Overfitting is when the model too is too heavily based on the training set. This results in the model only being able to classify the training data, but any new dat that it sees will likely be miss-classified. Ideally, a model will be **general** enough to fit the who dataspace, whilse being fitted enough to preform well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Set\n",
    "\n",
    "In order to try and know if we are overfitting our data, it is common to split the data into 2 sections, one that the we will train the model on, and the other that we will not use untill we are ready to test the model. \n",
    "\n",
    "This can be doinng using `sklearn` easily by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: We must never use the testing set for training. This will defeat the point of the whole exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is used to check how well a model is doing.\n",
    "\n",
    "It simply enumerates the number of \n",
    "- True Negatives (Good)\n",
    "- False Negatives (Bad)\n",
    "- False Positives (Good)\n",
    "- True Positives (Good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The accuracy of a model is the answer to the question \"Out of all of the points, how many did the model classify correctly?\"\n",
    "\n",
    "Using the Confusion Matrix, this is simply:\n",
    "\n",
    "$$\n",
    "a = \\frac{(True \\space Positives) + (True\\space Negatives)}{Total \\space number \\space of \\space data \\space points}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Regression Metrics\n",
    "\n",
    "There are many ways of mathematically seeing how far off a regression line is. \n",
    "\n",
    "1. Absolute Error\n",
    "2. Mean Squared Error\n",
    "3. R2-Score\n",
    "\n",
    "The absolute error is simple, it simply takes the sum of the absolite didtances between the actual points and the regression line. This is generally not a good metric, as it is not differentiable for Gradient Decent, and it doesnt penalise large errors.\n",
    "\n",
    "The Mean Squared Error is what we have preiviously used. It sums the square of the absolute distances, and then devides by the number of data points. This penailses big errors more than small errors.\n",
    "\n",
    "The R2-Score is new. What this model does is caclculates the average of the dataset to be a baseline \"worst case\". it then devides the Mean Squared Error of the regression line by this baseline and takes the value away from 1. This makes bad regression models have a score close to zero (not much better than the average guess), and good regression models have a score close to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Types of Error\n",
    "\n",
    "There are two types of errors that Machine Learning Models are suseptable to.\n",
    "\n",
    "**Underfitting** is over-simplifying the problem. In general, the model will be too simple to classify the data, and will usually not even preform well on the trianing set.\n",
    "\n",
    "**Overfitting** is the opposite of this, where the problem is over complicated so that the model is so specific to the training set, then it cannot classify new datapoints correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Model Decisons\n",
    "\n",
    "In order to make dicisions about out model, articulary whether it is overfitting, then we need to validate it against some unseen datapoint. However, the golden rule or machine larning is to not use your testing set for anything other than testing, *especalliy* not for making important decisions about the model as this would defeat the point of testing, and result in overfitting. \n",
    "\n",
    "Instead, we split our training set down again, into training and validation sets. This way, we can trin on the training set, validate and make dicisions based on the validation set, and finally test the model on the testing set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation\n",
    "It may seem that splitting off a testing set from our data is a waste, which is true! it may contain features that would improve our model massivly. The solution to this is K-fold cross validation.\n",
    "\n",
    "\n",
    "Instead of splitting out data into training/testing, we spllit it into k-sized buckets. We then train the model k times, each using a different bucket as the validation set. The model is then averaged\n",
    "\n",
    "In code:\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(size of data, validation bucket size)\n",
    "\n",
    "for train_indecies, test_indecies in kf:\n",
    "    .....\n",
    "```\n",
    "\n",
    "If you did not want distinct boxes that are next to eachother, then you could select the validation bucket from random places in the data\n",
    "\n",
    "```\n",
    "kf = KFold(size of data, validation bucket size, shuffle=True)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
