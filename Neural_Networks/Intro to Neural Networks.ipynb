{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression Overview\n",
    "\n",
    "Logistic Regression uses trends in the data in order to predict which class a new data-point belongs to.\n",
    "\n",
    "In order to find the line that cuts the data in the \"best\" place (i.e. splits the classes most clearly) we can use **Gradient Decent**. A line is drawn randomly, and the number of errors (or later, the error value) is measurd. Then, we preform gradient decent on this error value in order to minimise the error, resulting in a line that seperates the data well.\n",
    "\n",
    "The **Error Function** that was mentioned is more complex than simply counting the number of incorrectly classified points. Instead, it works on a penalty system, where correctly classified points recieve a small penaty, and incorrect point recieve a very large penalty. This system takes into account the whole data space, which is good. \n",
    "\n",
    "Gradient Decent is now applied to the system minimising the result of the error function. A simple implamentation of the error function could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_function(dataspace):\n",
    "    return sum(d.error for d in dataspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is not usual that a large dataspace can be split into correct classes using a single straight line. Often, multiple devisor line, curves, and circles may be needed to correctly classify a new point. \n",
    "\n",
    "Neural Networks are good at solving these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Basic Neural Network\n",
    "\n",
    "In the case of a dataspace being classified by two distinct lines, in order to classify a new point, we must check if it is in the area that is classified by the two lines. This is a complec operations, so it is possible to break it down. \n",
    "\n",
    "We could instead ask three simple questuins, and orientatie them into a Neural Network.\n",
    "\n",
    "1. Is the new point above the first line?\n",
    "2. Is the new point above the second line?\n",
    "3. Were the previous answers **BOTH** true?\n",
    "\n",
    "This problem is now well suited to Neural nets, seen in the diagram below\n",
    "\n",
    "INSERT DIAGRAM HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "Perceptrons, or *Neurons* are the simple nodes within a neural network. Each perceptron takes in some number of inupts, and decides what to send as a singular output.\n",
    "\n",
    "It quickly becomes apparent that not all of the inuts to the Perceptron hold the same importance, or ***weight***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight\n",
    "\n",
    "When a perceptron has several inputs, it must to be able to know which inputs are the most important, and hold the most weight over the output. Weights are initized to a **random** value, and then these weights are altered based on feedback, this is what is altered during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combining the Inputs\n",
    "\n",
    "Each perceptron summs the value from each input multiplied by its input weight. This forms the singular value that the percecptron operates on. This process is known as *linear combination*.\n",
    "\n",
    "$$\n",
    "total\\_input = \\sum{x_{i} w_{i}}\n",
    "$$\n",
    "\n",
    "or alternitavely, in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_input(list_of_inputs, list_of_weights):\n",
    "    total = 0\n",
    "    for i in range(len(list_of_inputs)):\n",
    "        total += list_of_inputs[i]*list_of_weights[i]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Activation Function\n",
    "\n",
    "The Activation function is what takes in the singulat value, and decides whether the nuron should activate or not. In this case, should return a $1$ or a $0$. \n",
    "\n",
    "Due to this abstracted nature, the activation function can be any function that takes a single input and returns a single value. A simple example is the **Heavyside Step** function, which returns *zero* if the input is less than zero, and one if it is greater or equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def heavy_side(x):\n",
    "    if (x<0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "A Bias is used to shift the result of the activation function so that the result is more suitable. Like the weights, the Bias can be initized to a random value, and then trained by the network.\n",
    "\n",
    "An example:\n",
    "\n",
    "$$\n",
    "f(x\\_ \\{ 1\\} ,...,x\\_ \\{ m\\} )\\quad =\\quad \\begin{matrix} 0\\quad if\\quad b+\\sum { { w }_{ i }{ x }_{ i }\\quad <\\quad 0 }  \\\\ 1\\quad if\\quad b+\\sum { { w }_{ i }{ x }_{ i }\\quad \\ge \\quad 0 }  \\end{matrix}\n",
    "$$\n",
    "\n",
    "Could be used to classify the university admiission example. The above activation function would return $1$ if they shoud be accepted, and $0$ otherwise. \n",
    "\n",
    "The weights and the Biases can then be updated to fit the data better with a learning algorithm such as Gradient Decent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The  AND Gate Neural Net\n",
    "\n",
    "\n",
    "Here we will create a Perceptron that mimics the rules of an AND gate for two inputs. As the solution is well defined an simple, we will manually set the correcct weights and Bias for the Perceptron.\n",
    "\n",
    "### The Activation Function\n",
    "\n",
    "In this example we will use a *Heavy-side Step* as the activation function. Remember that as there is a Bias, the formula looks like:\n",
    "\n",
    "$$\n",
    "f(x\\_ \\{ 1\\} ,...,x\\_ \\{ m\\} )\\quad =\\quad \\begin{matrix} 0\\quad if\\quad b+\\sum { { w }_{ i }{ x }_{ i }\\quad <\\quad 0 }  \\\\1 \\quad otherwise\\end{matrix}\n",
    "$$\n",
    "\n",
    "### The Weights\n",
    "\n",
    "As this is a straight AND gate, all of the input weights should be the same (The value can be arbitary, as long as the Bias compensates). For simplisity, we will set the weights to both $1$. \n",
    "\n",
    "### The Bias\n",
    "\n",
    "Knkowing that the only correct solution is when both inputs are 1, as are weights are 1, the total value of the Perceptron will be $b + 2$. If we want the resullt to be negative in any incorrect state (10,01,00) then we should take the Bias to be -2. \n",
    "\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -2.0                    0          Yes\n",
      "      0          1                  -1.0                    0          Yes\n",
      "      1          0                  -1.0                    0          Yes\n",
      "      1          1                   0.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "weight1 = 1.0   # Arbitary, but the same\n",
    "weight2 = 1.0   # Arbitary, but the same\n",
    "bias = -2.0     # In order to ensure all incorrect pairings are negative\n",
    "\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The OR Gate\n",
    "\n",
    "Simularly, the weights of the input vlaues must be equal, however this time we only want the result to be negative in the case where the inouts are (00). As a result, we can either slightly increase the weights of the inputs, or decrease the magnitgde of the Bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -1.0                    0          Yes\n",
      "      0          1                   0.0                    1          Yes\n",
      "      1          0                   0.0                    1          Yes\n",
      "      1          1                   1.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "weight1 = 1.0   # Arbitary, but the same\n",
    "weight2 = 1.0   # Arbitary, but the same\n",
    "bias = -1.0     # In order to ensure all incorrect pairings are negative\n",
    "\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, True, True, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The NOT Gate\n",
    "This Perceptron has a single input (ignore any except the first input), and just inverts whatever it is give.\n",
    "\n",
    "Do do this, we can simply have a weight of -1. This will have no effect of a input of 0, which the heavy side will then set as 1, but will make a plus 1 inout -1, meannig havyside willset it as 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      1          0                  -1.0                    0          Yes\n",
      "      0          0                   0.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "weight1 = -1.0  # Arbitary value, must be negative\n",
    "weight2 = 0.0   # All other inputs are ignored\n",
    "bias = 0.0     \n",
    "\n",
    "# Inputs and outputs\n",
    "test_inputs = [(1, 0), (0, 0)]\n",
    "correct_outputs = [False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "# A Simple Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Activation Functions\n",
    "\n",
    "As we mentioned earlier, the architecture of the Perceptron means that We can define any *Activation Function* that simply takes the result form the Linear COmbination of the input, and returns a single value. There are several common ones that are more sophisticated than the Heavy-side Step that we used earlier. \n",
    "\n",
    "The one that we wll be using most is called a **Sigmoid Function**. The idea of a sigmoid is that it smooths out the range, with limits at 0 and 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+(np.e**(-x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The difference between Regression and Neural Networks?\n",
    "\n",
    "Up until this point, there has been very little differencebetween the capability of the Regression model and the Neural nets that we have been looking at; and too an extent, a single Perceptron is essentailly the same as a Regression model if the Activation function is a certain way. However, when we start to stack multiple Perceptrons together to form networks and layers, we can begin to handel *Linearly Inseperable* data, which is something that Regression cannot do. \n",
    "\n",
    "A further advantage is the flexability that the Activation Function allows us. If our net uses continuous and Differentiable functions, then we can use learning Algorithms to train the network, for example using Gradient Decent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Learning Weights\n",
    "\n",
    "For any system to learn, it must have a sense of **Trial and Error**. This error is needed to know when the network is wrong, and to know whether it is getting better, or diverging from the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error\n",
    "\n",
    "The Error that we are refuring to must obviousl be mathematically defined, and it turns out that a good measure is the **Sum of Squared Errors**. This formula is good for several reasons, it take into account the whole dataset, and also penalises larger errors more than smaller errors, as the error value is squared. Squaring the value also has the advantage of meaning that all errors are positive. \n",
    "\n",
    "$$\n",
    "E\\quad =\\quad \\frac { 1 }{ 2 } \\sum _{  }^{ \\mu  }{ \\sum _{  }^{ j }{ { \\left[ { y }_{ j }^{ \\mu  }-{ \\hat { y }  }_{ j }^{ \\mu  } \\right]  }^{ 2 } }  } \n",
    "$$\n",
    "Where $\\hat {y} $ in the above fromula is the result from the network, and $y$ is the expected value.\n",
    "\n",
    "\n",
    "In words, what the above equasion is doing is taking the differnce between each output node in the network and its expected value, and squaring it, then sum it. Now, you do that for all data points and sum up those values too.\n",
    "\n",
    "\n",
    "This results in a error that encompasses the error for all of the output nodes under all datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error vs Weights\n",
    "\n",
    "If you recall, the output from a perceptron is dependent on the weight of the inputs. In turn, the error of the Perceptron is hense also indirectly dependent on these weights.\n",
    "\n",
    "This is exactly what we want! \n",
    "\n",
    "When we need to rekduce error, we know that the buttons to press are the weights of the Inputs.\n",
    "\n",
    "This can be seen when we rewrite the nets output of $y$ in its own derivation.\n",
    "\n",
    "$$E\n",
    "\\quad =\\quad \\frac { 1 }{ 2 } \\sum _{  }^{ \\mu  }{ \\sum _{  }^{ j }{ { \\left[ { y }_{ j }^{ \\mu  }-{ f(\\sum _{  }^{ i }{ { w }_{ ij } } \\hat { y }  }_{ j }^{ \\mu  }) \\right]  }^{ 2 } }  } \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent\n",
    "\n",
    "The main idea behind Gradient decent it to take lots of small steps in the direction that minimises the desited variable.\n",
    "\n",
    "In this case, the term 'gradient' means the slope of the function at the curent point.\n",
    "\n",
    "As we know, the slope of a function is calculated ising the origonal functions derivitive. \n",
    "\n",
    "The main problem with the gradient decent method is that by definition it will never go uphill, so is suseptable to local minimums. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can be done about this?\n",
    "\n",
    "We can run the network on data with a known output, and taylor the input weights to suit that outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I change the weight?\n",
    "\n",
    "It is known that the Error is a function of the Weights of the input nodes. Concider a case where there is only one input (and hense weight) involved, then it can be thought that:\n",
    "\n",
    "$$\n",
    "\\Delta w \\propto -gradient\n",
    "$$\n",
    "\n",
    "The change in the weight will be in the opposite direction to the gradient. Whach makes sense, if the gradient is positive, then going left will be down hill\n",
    "\n",
    "It follows that:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta w &\\propto -\\frac {\\delta E  }{\\delta w  } \\\\\n",
    "\\Delta w &= -\\eta \\frac {\\delta E  }{\\delta w  } \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The scalling constant $\\eta$ is known as the learning rate, and it simply changes how quickly the weights change for a given error. Smaller takes longer, but bigger can struggle to settle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is that Partial Derivitive?\n",
    "\n",
    "In order to calculate the change in the weight for each input, then we need to solve the differental:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\delta E  }{\\delta w  }& = \\frac {\\delta  }{\\delta w  }\\frac{1}{2}(y-\\hat{y})^2\\\\\n",
    "\\text{as } \\space \\hat{y} \\space\\text{is a function of} \\space w\\\\\n",
    "& = \\frac {\\delta  }{\\delta w  }\\frac{1}{2}(y-\\hat{y}(w))^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using the chain rule, the squared comes down and cancels witht the half, and we multiply be the derivitive of  $y-\\hat{y}$ with respect to $w$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\delta E  }{\\delta w  }& = (y-\\hat{y})  \\frac {\\delta   }{\\delta w  } (y-\\hat{y})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As the $y$ and the minus are not dependent on the wight (What we are deriving with respect to) then the y can go away and the minus is brought outside \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\delta E  }{\\delta w  }& = -(y-\\hat{y})  \\frac {\\delta  \\hat{y} }{\\delta w  }\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, if you recall that the predicted weight $\\hat{y}$ is the result of applying the activation function to the Linear combination :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} &= f(h) \\\\\n",
    "h &= \\sum{w_i x_i} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\delta E  }{\\delta w  }& = -(y-\\hat{y})  \\frac {\\delta  \\hat{y} }{\\delta w  }\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "by the chain rule, becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\delta E  }{\\delta w_i  }& = -(y-\\hat{y}) f'(h)  \\frac {\\delta  }{\\delta w  } \\sum{w_i x_i}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally by the derivitive of summations,\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\delta E  }{\\delta w_i  }& = -(y-\\hat{y}) f'(h)  x_i\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we do witht the partal derivitive?\n",
    "\n",
    "Now that we have differentiated the Error with respect to the weights properly, then we can work backwards by saying that:\n",
    "$$\n",
    "\\\\\n",
    "\\Delta w = \\eta (y-\\hat{y}) f'(h)  x_i\\\\\n",
    "$$\n",
    "\n",
    "So we know have the change in the weight for each input $x_i$.\n",
    "\n",
    "To simplify the above, we say that Define an **Error Term** $\\delta$:\n",
    "$$\n",
    "\\delta = (y-\\hat{y}) f'(h) \n",
    "$$\n",
    "\n",
    "So we now see that \n",
    "\n",
    "$$\n",
    "\\Delta w_i = \\eta \\delta  x_i\\\\\n",
    "$$\n",
    "\n",
    "So **FINALLY** :\n",
    "\n",
    "In order to update the weight\n",
    "\n",
    "$$\n",
    "w = w + \\eta \\delta  x_i\\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Gradient Decent in Code\n",
    "\n",
    "Compiling the above:\n",
    "\n",
    "The change in weight is $\\Delta w_i = \\eta \\delta  x_i$\n",
    "\n",
    "Where the error function is $(y-\\hat{y}) f'(h)$\n",
    "\n",
    "Which is eqivilent to $\\delta = (y-\\hat{y}) f'(\\sum{w_i x_i})$ \n",
    "\n",
    "\n",
    "In the above, $(y-\\hat{y}$ is equivilent to the output error.\n",
    "\n",
    "We will now define the system in codeassuming a single output unit, and a sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "Here is the Sigmoid function, and the function that returns the dreivitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change in Weight\n",
    "\n",
    "This is where we use GD to calculate a change in weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.689974481128\n",
      "Amount of Error:\n",
      "-0.189974481128\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "learnrate = 0.5\n",
    "inputs = np.array([1, 2, 3, 4])\n",
    "target = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "weights = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "\n",
    "\n",
    "# The node's linear combination of inputs and weights\n",
    "#     This is a single value\n",
    "linear_comb = np.dot(inputs, weights) \n",
    "\n",
    "# The output of neural network\n",
    "nn_output = sigmoid(linear_comb)\n",
    "\n",
    "# The error of neural network\n",
    "#      This is y - y-hat\n",
    "error = target - nn_output\n",
    "\n",
    "# Output Gradient\n",
    "output_gradient = sigmoid_prime(linear_comb)\n",
    "\n",
    "# The error term\n",
    "error_term = error * output_gradient\n",
    "\n",
    "#The final change in weights\n",
    "del_w = learnrate * error_term * inputs\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Multi updates\n",
    "\n",
    "For this example we will be using the uni admissions example. \n",
    "\n",
    "The data has 3 columns. \n",
    "1. GRE Score\n",
    "2. GPA\n",
    "3. Shcool Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clean Up\n",
    "\n",
    "It is clear that the `rank` of the school is not a mathematical value, and encodes no meaning. \n",
    "\n",
    "Because of this, we use **Dummy Variables** to show the school instead. \n",
    "\n",
    "We will use 4 Dummy columns to represent the 4 rank schools, the value in the column representing the schools rank will be $1$, all other `rank` columns will be $0$.\n",
    "\n",
    "As we are again using a **Sigmoid activation function** must normaise the data to have a mean of $0$ and a standard deviation of $1$. This is because the sigmoid function squishes big and small values to 1 and 0, if this happens then the gradient will be nearly 0, meaning that the network will struggle to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of Square Error\n",
    "\n",
    "Previously we have used the *Sum of Squared Errors* to calculte Error. However in this example we will use the ***Mean* of Squared Errors**. The only difference is that we devide by the number of data points, $m$. \n",
    "\n",
    "The reason is that as there is a large number of data points, the sum of the error is large, resulting in large steps, which is problematic for Gradient Decent. To combat this, ti would be possible to just use a smaller learning Rate. However as the number of points is constant, division by $m$ has the same effect, without us having to change the Learning Rate.\n",
    "\n",
    "$$\n",
    "E\\quad =\\quad \\frac { 1 }{ 2m } \\sum _{  }^{ \\mu  }{ { \\left[ { y }_{ j }^{ \\mu  }-{ \\hat { y }  }_{ j }^{ \\mu  } \\right]  }^{ 2 } }  \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implamentation\n",
    "\n",
    "The General steps are as follows.\n",
    "\n",
    "1. Set the inital $\\Delta w_i = 0$\n",
    "2. For each record in the data\n",
    " i. Make a forward pass though the netwrok, calculating the output $\\hat{y}$\n",
    " ii. Calculate the *Error Term*, $\\delta$\n",
    " iii. Update $\\Delta w_i$\n",
    "    \n",
    "3. Update the weight, $w_i = \\frac {w_i + \\eta \\Delta w_i}{m}$\n",
    "4. Repeat for all epochs\n",
    "\n",
    "\n",
    "**Note**: this will not run withough the data_prep, but it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# The Random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000                               # What are Epochs?\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    # Initalise the intal change in weight to be Zero\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    \n",
    "    for inpt, target in zip(features.values, targets):\n",
    "        # Loop through all records\n",
    "        \n",
    "        #Linear Combination\n",
    "        linear_comb = np.dot(inpt, weights)\n",
    "\n",
    "        output = sigmoid(linear_comb)\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = (target - output)\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error * (output*(1-output))\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += error_term*inpt\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += learnrate*del_w/n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Hidden Layer\n",
    "\n",
    "We saw in the first example (AND, XOR) that stacking multiple perceptron layers allowed us to classify **Linearly Inseperable** things. This is where Deep Learning Gets its name, thenwetwroks are multiple layers deep.\n",
    "\n",
    "Our weigths are no longer a vector, but rather a matrix where the i and j indexes represent the inout and hidden layers respectivly.\n",
    "\n",
    "For example, assuming that `features` is the 2D matrix of input data:\n",
    "\n",
    "```\n",
    "\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "\n",
    "So now, we need to do matrix multiplication to find the linear combinations.\n",
    "\n",
    "```\n",
    "hiden_inputs = np.matmul(inputs, weights_input_to_hidden)\n",
    "```\n",
    "\n",
    "**Pro-Tip**: to create a column vector from a row, usearr[:,None]\n",
    "\n",
    "**TODO**\n",
    "\n",
    "1. Calculate the input to the hidden layer.\n",
    "2. Calculate the hidden layer output.\n",
    "3. Calculate the input to the output layer.\n",
    "4. Calculate the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[ 0.41492192  0.42604313  0.5002434 ]\n",
      "Output-layer Output:\n",
      "[ 0.49815196  0.48539772]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make some fake data\n",
    "Input = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(Input, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Back Propagation\n",
    "\n",
    "This is the idea that even with multiple hidden layers, the error can be propagated back thought the network using the weights so that the whole network can be trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Input = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(Input, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target-output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error*(output*(1-output))\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * hidden_layer_output*(1-hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * Input[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
