{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniFlow\n",
    "\n",
    "In this lesson we will be building a machine learning library called **MiniFlow**. Miniflow will be used as an example to highlight the abstraction principles that **TensorFlow** will providein the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "A neural net can be thought of a mathematical graph, in which each node is a function, and the edges are the weights.\n",
    "\n",
    "### Forward Propagation\n",
    "We have seen plenty of times now what forward propagation is. A set of input values travels forward through then graph and leaves as an output.\n",
    "\n",
    "A network is created by defining nodes, and propagating values. These are therefore two functions that MiniFlow must posess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  MiniFlow Architecture\n",
    "\n",
    "MiniFlow will use Object orientated Python.\n",
    "\n",
    "Here we will define a Node, and several speclised subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"\n",
    "    A Generic Base Node.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        A nodes initalization.\n",
    "        \n",
    "        Args:\n",
    "            inbound_nodes: A list of nodes that input to this node on a forward pass\n",
    "        \"\"\"\n",
    "        self.inbound_nodes  = inbound_nodes        # Where the inputs come from\n",
    "        self.outbound_nodes = []                   # Where the outputs are going \n",
    "        \n",
    "        for node in self.inbound_nodes:            # Settting the network up\n",
    "            node.outbound_nodes.append(self)\n",
    "            \n",
    "        self.value = None                          # Where the output will be stored\n",
    "        self.gardient = {}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        The forward propagation function.\n",
    "        \n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplamentedError\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        The backward propagation function.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplamentedError\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    An Input Node.\n",
    "    \n",
    "    This class of nodes has no inbound nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The Input Nodes initalizaton.\n",
    "        \n",
    "        As this class of node has no inbound nodes, then it passes nothing to the superclass,\n",
    "        as the default there is `[]`\n",
    "        \"\"\"\n",
    "        Node.__init__(self)\n",
    "    \n",
    "    def forward(self, value=None):\n",
    "        \"\"\"\n",
    "        The method that initates a forward pass.\n",
    "        \n",
    "        This is the only node class that should be passed a value to forward,\n",
    "        the rest should take it from their inbound_nodes\n",
    "        \"\"\"\n",
    "        if value != None:\n",
    "            self.value = value\n",
    "            \n",
    "    def backward(self):\n",
    "        self.gardients = {self:0}\n",
    "        \n",
    "        for node in self.outbound_nodes:\n",
    "            other_cost = node.gradients[self]\n",
    "            self.gradients[self] += grad_cost * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    \"\"\"\n",
    "    An Adding Node.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *inputs):\n",
    "        \"\"\"\n",
    "        The Add Nodes initalizaton.\n",
    "        \n",
    "        As this class of node has no inbound nodes, then it passes nothing to the superclass,\n",
    "        as the default there is `[]`\n",
    "        \"\"\"\n",
    "        Node.__init__(self, inbound_nodes=inputs)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Adds the values of all the input nodes\n",
    "        \n",
    "        Sets this nodes value to the sum of that of all input nodes.        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.value = sum([val.value for val in self.inbound_nodes] )\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mul(Node):\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        For reference, here's the old way from the last\n",
    "        quiz. You'll want to write code here.\n",
    "        \"\"\"\n",
    "        self.value = reduce(lambda x, y: x*y, [val.value for val in self.inbound_nodes])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some of the architecture sorted out, we can begin with think about how they will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns:\n",
    "        A list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Args:\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns:\n",
    "        The output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 + 5 + 10 = 19 (according to miniflow)\n",
      "4 * 5 * 10 = 200 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "t,u,v = Input(), Input(), Input()\n",
    "\n",
    "f = Add(x, y, z)\n",
    "m = Mul(t,u,v)\n",
    "\n",
    "feed_dict = {x: 4, y: 5, z: 10}\n",
    "\n",
    "feed_dict_2 = {t: 4, u: 5, v: 10}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "graph2 = topological_sort(feed_dict_2)\n",
    "output = forward_pass(f, graph)\n",
    "output_2 = forward_pass(m, graph2)\n",
    "\n",
    "# should output 19\n",
    "print(\"{} + {} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], feed_dict[z], output))\n",
    "print(\"{} * {} * {} = {} (according to miniflow)\".format(feed_dict_2[t], feed_dict_2[u], feed_dict_2[v], output_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        w = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        \n",
    "        self.value = sum((X[i]*w[i]) for i in range(len(X))) + b\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Edges Between Layers\n",
    "\n",
    "The idea of converiting an input to an output in multiple dimensions can be done using Linear Algebra.\n",
    "A **Transform** does exactly this.\n",
    "\n",
    "If we go back to the output function of the Linear Node:\n",
    "$$\n",
    "y=\\sum { w_{ i }x_{ i }\\quad  } +\\quad b\n",
    "$$\n",
    "\n",
    "It is knon that the inuts and the weights are not necisarrily lists, and in fact are most likely to be tensors.\n",
    "For this reason, we must redefine `Linear` to be able to handel tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value # This is a ndarray\n",
    "        W = self.inbound_nodes[1].value # This is a ndarray\n",
    "        b = self.inbound_nodes[2].value # This is a vector\n",
    "        \n",
    "        self.value = np.matmul(X,W) + b\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid Function\n",
    "\n",
    "This Sigmoid Function is amongst the most common activation functions in neural networks for deep learning.\n",
    "\n",
    "$$\n",
    "sigmoid(x)\\quad =\\quad \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "Sigmoid mimics the behaviour of the simple step functions that are used in preceptrons, whilst still being (Very easily) differentiable. This trait allows gradient decent to work.\n",
    "\n",
    "Sigmoid takes a single input, and is an activation function (non-linerarity), meaning that it takes its input, preforms some operation, and produces an output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    This sigmoid activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_node):\n",
    "        Node.__init__(self, [input_node])\n",
    "        \n",
    "    def _sigmoid(self, X):\n",
    "        return 1.0/(1.0+np.e**(-X))\n",
    "    \n",
    "    def forward(self):\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value  = self._sigmoid(input_value)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost, Loss, and Accuracy\n",
    "\n",
    "These terms are often used somewhat interchangably if refernce to how well a network is doing. We will use **Cost** here. \n",
    "\n",
    "In this example we will be using the **Mean Squared Error** as the definition of Cost.\n",
    "\n",
    "The cost refers to the error that is present as a result of using paticcular weights and biases.\n",
    "\n",
    "$$\n",
    "Cost(w,b)\\quad = \\quad \\frac{1}{m}\\sum _{ x }^{  }{ { (y(x) - a) }^{ 2 } } \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        #m = self.inbound_nodes[0].value.shape[0]       # This is the number of rows of the matrix, its length\n",
    "                                                        # This is not needed as the np.mean method calculates \n",
    "                                                        # it internally  \n",
    "        \n",
    "        self.diff = y-a\n",
    "        \n",
    "        self.value = np.mean(self.diff**2)\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "\n",
    "        This is the final node of the network so outbound nodes\n",
    "        are not a concern.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    return x - learning_rate * gradx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "We are now ready to implemt back propagation.\n",
    "To do this, Node has recieved a new method, `backward` as well as another atribute, `self.gradients`, a dictionary that is used to cache gradients.\n",
    "\n",
    "Another helper function has been added, the `forward_and_backward`. This preforms a forward pass folloed by a backward pass.\n",
    "\n",
    "Above the backard methods have been implamented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
